{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_bert_(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzODz9UnLG9l",
        "colab_type": "code",
        "outputId": "2002bca6-6a0b-4cf8-d586-a59790095936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import re\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from copy import deepcopy\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer \n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import preprocessor as p"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/f8/810ec35c31cca89bc4f1a02c14b042b9ec6c19dd21f7ef1876874ef069a6/tweet-preprocessor-0.5.0.tar.gz\n",
            "Building wheels for collected packages: tweet-preprocessor\n",
            "  Building wheel for tweet-preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tweet-preprocessor: filename=tweet_preprocessor-0.5.0-cp36-none-any.whl size=7947 sha256=559ddd6783055c7a584018f6681dfa1eb827038f324de0343652a6227f12c8aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/27/cc/49938e98a2470802ebdefae9d2b3f524768e970c1ebbe2dc4a\n",
            "Successfully built tweet-preprocessor\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.5.0\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AimXkFvuLIp_",
        "colab_type": "code",
        "outputId": "ea2c71aa-0291-4318-93ab-895f22d048d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqF_j4oJLKZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/WebProject/labeledTrainData.csv\", delimiter=\"\\t\",names=[\"tweet\", \"label\", \"labelValue\"])\n",
        "#data = pd.read_csv(\"/content/drive/My Drive/WebProject/preprocessed_tweets.csv\", delimiter=\"\\t\",names=[\"tweet\", \"label\", \"labelValue\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmTPyGQkLP08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[\"label\"].replace({\"spam\": \"neutral\", \"normal\": \"neutral\"}, inplace=True)\n",
        "data[\"label\"].replace({\"neutral\":1, \"abusive\": 2,\"hateful\":3}, inplace=True)\n",
        "\n",
        "#data.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypCPbXC3_Wda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_tweet(tweet):\n",
        "\n",
        "    # 1. Remove URLs, emojis, mentions, smileys using tweet-preprocessor library\n",
        "    # should not remove hashtags as hashtags contains important tweet content.\n",
        "    \n",
        "    #p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION,p.OPT.RESERVED,\tp.OPT.SMILEY,p.OPT.NUMBER)\n",
        "    clean = p.clean(tweet)\n",
        "    # 2. Remove HTML tags using Beautiful soup library\n",
        "    no_tag = BeautifulSoup(clean).get_text()\n",
        "\n",
        "    # 3. Expanding hashtags.\n",
        "    \n",
        "    #tweets_expanded = expand_hashtags(no_tag)\n",
        "\n",
        "    # 4. Remove non letter char using re\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\",\" \", no_tag)\n",
        "    #letters_only = re.sub(\"[^a-zA-Z]\",\" \", tweets_expanded)\n",
        "    \n",
        "    # 5. Convert to lower case\n",
        "    lower_case = letters_only.lower()\n",
        "    words = lower_case.split()\n",
        "\n",
        "    new_words = []\n",
        "    ps = PorterStemmer() \n",
        "    for w in words:\n",
        "        new_words.append(ps.stem(w))\n",
        "    # for w in words:\n",
        "    #     new_words.append(w)\n",
        "    # print(new_words[0:4])\n",
        "    \n",
        "    # 5. Remove stop words\n",
        "    stops = set(stopwords.words(\"english\")) \n",
        "    final_words = [w for w in new_words if not w in stops]\n",
        "    # final_words = new_words\n",
        "\n",
        "    return(\" \".join( final_words ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3DDQePG_vvR",
        "colab_type": "code",
        "outputId": "a4a405e9-29e2-4b3c-8fdc-b0a95da697ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data['tweet'][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Beats by Dr. Dre urBeats Wired In-Ear Headphones - White https://t.co/9tREpqfyW4 https://t.co/FCaWyWRbpE'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB04tgec_XoB",
        "colab_type": "code",
        "outputId": "7f970201-b938-4d3f-8f5a-828eda26806f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "num_tweets = data[\"tweet\"].size\n",
        "# print(num_tweets)\n",
        "for i in range(num_tweets):\n",
        "    data[\"tweet\"][i] = preprocess_tweet(data[\"tweet\"][i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdcg_Inki_hX",
        "colab_type": "code",
        "outputId": "5ebd3b38-deea-4cce-eac7-f95a4763cf86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>labelValue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>beat dr dre urbeat wire ear headphon white</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>man would fuck rule parti wa perpetu warfar</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>time draw close father draw near alway</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>notic start act differ distant bc peep someth ...</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>forget unfollow believ grow new follow last da...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  label  labelValue\n",
              "0         beat dr dre urbeat wire ear headphon white      1           4\n",
              "1        man would fuck rule parti wa perpetu warfar      2           4\n",
              "2             time draw close father draw near alway      1           4\n",
              "3  notic start act differ distant bc peep someth ...      1           5\n",
              "4  forget unfollow believ grow new follow last da...      1           3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zXBCPXxnPmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "880UTQSUnSFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.drop(columns=\"labelValue\", inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5mjKXDqm4hx",
        "colab_type": "code",
        "outputId": "95bfb56d-0fe3-4015-e85c-7a1a8dbbf0cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>beat dr dre urbeat wire ear headphon white</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>man would fuck rule parti wa perpetu warfar</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>time draw close father draw near alway</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>notic start act differ distant bc peep someth ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>forget unfollow believ grow new follow last da...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  label\n",
              "0         beat dr dre urbeat wire ear headphon white      1\n",
              "1        man would fuck rule parti wa perpetu warfar      2\n",
              "2             time draw close father draw near alway      1\n",
              "3  notic start act differ distant bc peep someth ...      1\n",
              "4  forget unfollow believ grow new follow last da...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y03L3EwU1g8A",
        "colab_type": "code",
        "outputId": "7dc31ac9-17fc-40eb-e314-5e75568e0914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "data['tweet_length'] = [len(text.split(' ')) for text in data.tweet]\n",
        "\n",
        "data['tweet_length'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9     10983\n",
              "10    10893\n",
              "8     10645\n",
              "7      9894\n",
              "11     9528\n",
              "6      8373\n",
              "12     7532\n",
              "5      6969\n",
              "13     5672\n",
              "4      5660\n",
              "3      3423\n",
              "14     3400\n",
              "2      2104\n",
              "15     1955\n",
              "16     1081\n",
              "1       856\n",
              "17      572\n",
              "18      244\n",
              "19      115\n",
              "20       54\n",
              "22       14\n",
              "21       13\n",
              "23        6\n",
              "24        5\n",
              "25        2\n",
              "30        2\n",
              "26        1\n",
              "Name: tweet_length, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nKl3jp72-FW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data = data[data['tweet_length']>3]\n",
        "\n",
        "data = data.drop_duplicates(subset=['tweet'])\n",
        "\n",
        "data['tweet_BERT'] = '[CLS] '+data.tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oGQhW-635v7",
        "colab_type": "code",
        "outputId": "c931272c-b99c-49a9-d2c6-d7e935a9a3b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "82027"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUdEzfFGj201",
        "colab_type": "code",
        "outputId": "49faa0c1-d3a8-46c2-bd9e-02b57980d6fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.12.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.38.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.15.43)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.43->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.43)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NNGMtm_3Jtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "data['tweet_BERTbase_length'] = [len(tokenizer.tokenize(sent)) for sent in data.tweet_BERT]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAcMY_Cd170-",
        "colab_type": "code",
        "outputId": "8c7521f1-2da3-4733-813e-d694e024ae1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet_length</th>\n",
              "      <th>tweet_BERT</th>\n",
              "      <th>tweet_BERTbase_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>beat dr dre urbeat wire ear headphon white</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>[CLS] beat dr dre urbeat wire ear headphon white</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>man would fuck rule parti wa perpetu warfar</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>[CLS] man would fuck rule parti wa perpetu warfar</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>time draw close father draw near alway</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>[CLS] time draw close father draw near alway</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>notic start act differ distant bc peep someth ...</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>[CLS] notic start act differ distant bc peep s...</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>forget unfollow believ grow new follow last da...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>[CLS] forget unfollow believ grow new follow l...</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  ...  tweet_BERTbase_length\n",
              "0         beat dr dre urbeat wire ear headphon white  ...                     12\n",
              "1        man would fuck rule parti wa perpetu warfar  ...                     13\n",
              "2             time draw close father draw near alway  ...                      9\n",
              "3  notic start act differ distant bc peep someth ...  ...                     19\n",
              "4  forget unfollow believ grow new follow last da...  ...                     16\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW5Wr8cjmGIg",
        "colab_type": "code",
        "outputId": "898d2415-a3d8-4e16-f257-03c8cd80d00f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "label_dict = dict()\n",
        "for i, l in enumerate(list(data.label.value_counts().keys())):\n",
        "    label_dict.update({l: i})\n",
        "# for each unique label, assign a numeric identiifer\n",
        "data['type_label'] = [label_dict[i] for i in data.label] #create a column in df to store the numeric ids\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet_length</th>\n",
              "      <th>tweet_BERT</th>\n",
              "      <th>tweet_BERTbase_length</th>\n",
              "      <th>type_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>beat dr dre urbeat wire ear headphon white</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>[CLS] beat dr dre urbeat wire ear headphon white</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>man would fuck rule parti wa perpetu warfar</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>[CLS] man would fuck rule parti wa perpetu warfar</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>time draw close father draw near alway</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>[CLS] time draw close father draw near alway</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>notic start act differ distant bc peep someth ...</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>[CLS] notic start act differ distant bc peep s...</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>forget unfollow believ grow new follow last da...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>[CLS] forget unfollow believ grow new follow l...</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  ...  type_label\n",
              "0         beat dr dre urbeat wire ear headphon white  ...           0\n",
              "1        man would fuck rule parti wa perpetu warfar  ...           1\n",
              "2             time draw close father draw near alway  ...           0\n",
              "3  notic start act differ distant bc peep someth ...  ...           0\n",
              "4  forget unfollow believ grow new follow last da...  ...           0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuQfgO58MgYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hateful = data[\"label\"]==3\n",
        "hateful = data[hateful]\n",
        "hateful = hateful.sample(frac=1)\n",
        "hateful = hateful.reset_index(drop=True)\n",
        "# hateful = hateful[:4333]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJjFKw8-NFpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "abusive = data[\"label\"]==2\n",
        "abusive = data[abusive]\n",
        "abusive = abusive.sample(frac=1)\n",
        "abusive = abusive.reset_index(drop=True)\n",
        "# abusive = abusive[:4333]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXwW--AmNMqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neutral = data[\"label\"]==1\n",
        "neutral = data[neutral]\n",
        "neutral = neutral.sample(frac=1)\n",
        "neutral = neutral.reset_index(drop=True)\n",
        "# neutral = neutral[:4334]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3OG7vjXOTkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "union = pd.concat([hateful, abusive,neutral])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGf1CQXkOpAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "union = union.sample(frac=1)\n",
        "union = union.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oms4I5UPDkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = union[:69996]\n",
        "validation_data = union[69996:79996]\n",
        "test_data = union[79996:99996]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1KMy4ZIU0Q3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_data = union[:10000]\n",
        "# validation_data = union[10000:11000]\n",
        "# test_data = union[11000:13000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAtEFFQ5kxAb",
        "colab_type": "code",
        "outputId": "a44745f9-a20f-4c9a-f78b-7e6a826f8798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "train_data[:1000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet_length</th>\n",
              "      <th>tweet_BERT</th>\n",
              "      <th>tweet_BERTbase_length</th>\n",
              "      <th>type_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mc student taylor introduc tutor train worksho...</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>[CLS] mc student taylor introduc tutor train w...</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>memo big brand declar victori advertis youtub</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>[CLS] memo big brand declar victori advertis y...</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hear sale go everyth</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>[CLS] hear sale go everyth</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>queen get frustrat fuck bc go say someth get d...</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>[CLS] queen get frustrat fuck bc go say someth...</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hate peopl like fight ugli ugli nigga shoot in...</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>[CLS] hate peopl like fight ugli ugli nigga sh...</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>wa wonder ani detail soquel event june th book...</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>[CLS] wa wonder ani detail soquel event june t...</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>thought build monster univers back godzilla sk...</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>[CLS] thought build monster univers back godzi...</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>preet bharara hyperlink fire trump group helte...</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>[CLS] preet bharara hyperlink fire trump group...</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>bout send number bro bro lost ya new number ap...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>[CLS] bout send number bro bro lost ya new num...</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>hello u hear scream mean ask help aaaaaaaaaaaa...</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>[CLS] hello u hear scream mean ask help aaaaaa...</td>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 tweet  ...  type_label\n",
              "0    mc student taylor introduc tutor train worksho...  ...           0\n",
              "1        memo big brand declar victori advertis youtub  ...           0\n",
              "2                                 hear sale go everyth  ...           0\n",
              "3    queen get frustrat fuck bc go say someth get d...  ...           1\n",
              "4    hate peopl like fight ugli ugli nigga shoot in...  ...           2\n",
              "..                                                 ...  ...         ...\n",
              "995  wa wonder ani detail soquel event june th book...  ...           0\n",
              "996  thought build monster univers back godzilla sk...  ...           0\n",
              "997  preet bharara hyperlink fire trump group helte...  ...           0\n",
              "998  bout send number bro bro lost ya new number ap...  ...           0\n",
              "999  hello u hear scream mean ask help aaaaaaaaaaaa...  ...           0\n",
              "\n",
              "[1000 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCKq5VeJj6XL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transformers\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "import math\n",
        "from typing import List\n",
        "from torch.nn.utils.rnn import pack_padded_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m8PuEmvhrYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_iter(data, batch_size, shuffle=False, bert=None):\n",
        "    \"\"\" Yield batches of sentences and labels reverse sorted by length (largest to smallest).\n",
        "    @param data (dataframe): dataframe with ProcessedText (str) and label (int) columns\n",
        "    @param batch_size (int): batch size\n",
        "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
        "    @param bert (str): whether for BERT training. Values: \"large\", \"base\", None\n",
        "    \"\"\"\n",
        "    batch_num = math.ceil(data.shape[0] / batch_size)\n",
        "    index_array = list(range(data.shape[0]))\n",
        "\n",
        "    if shuffle:\n",
        "        data = data.sample(frac=1)\n",
        "\n",
        "    for i in range(batch_num):\n",
        "        indices = index_array[i * batch_size: (i + 1) * batch_size]\n",
        "\n",
        "        if bert:\n",
        "            examples = data.iloc[indices].sort_values(by='tweet_BERTbase_length', ascending=False) #by='ProcessedText_BERT'+bert+'_length'\n",
        "            sents = list(examples.tweet_BERT)\n",
        "        else:\n",
        "            examples = data.iloc[indices].sort_values(by='tweet_length', ascending=False)\n",
        "            sents = [text.split(' ') for text in examples.tweet]\n",
        "\n",
        "        targets = list(examples.type_label.values)\n",
        "        yield sents, targets  # list[list[str]] if not bert else list[str], list[int]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAG08hRijomt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM_bert(torch.nn.Module):\n",
        "    def __init__(self, num_class, dropout_rate, bert_config='bert-base-uncased'):\n",
        "        \"\"\"\n",
        "        :param num_class: int, number of categories\n",
        "        :param bert_config: str, BERT configuration description\n",
        "        :param dropout_rate: float\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LSTM_bert, self).__init__()\n",
        "        self.num_class = num_class\n",
        "        self.bert_config = bert_config\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.bert_config)          \n",
        "        self.bert = BertModel.from_pretrained(self.bert_config)\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.lstm_input_size = self.bert.config.hidden_size\n",
        "        self.lstm_hidden_size = self.bert.config.hidden_size\n",
        "\n",
        "        self.lstm = torch.nn.LSTM(input_size = self.lstm_input_size,\n",
        "                                  hidden_size = self.lstm_hidden_size,\n",
        "                                  bidirectional = True)\n",
        "        \n",
        "        self.dropout = torch.nn.Dropout(p=self.dropout_rate)\n",
        "        self.fc = torch.nn.Linear(in_features = 2*self.lstm_hidden_size, #LSTM stacked hidden state\n",
        "                                  out_features = self.num_class, \n",
        "                                  bias=True)\n",
        "    \n",
        "    def forward(self, sents):\n",
        "        \"\"\"\n",
        "        :param sents: list[str], list of untokenized sentences \n",
        "        :return: torch.tensor of shape (batch_size, num_class)\n",
        "        \"\"\"\n",
        "        sents_tensor, masks_tensor, sents_lengths = sents_to_tensor(self.tokenizer, sents)\n",
        "        # 1. The tweet is first input to the model\n",
        "        encoded_layers, pooled_output = self.bert(input_ids=sents_tensor, attention_mask=masks_tensor, output_all_encoded_layers=False)\n",
        "        # 2. The output is reshuffled to the correct format\n",
        "        encoded_layers = encoded_layers.permute(1, 0, 2) #permute dimensions to fit LSTM input\n",
        "        # 3. The encoded layers are fed into the biLSTM\n",
        "        enc_hiddens, (last_hidden, last_cell) = self.lstm(pack_padded_sequence(encoded_layers, sents_lengths))\n",
        "        \n",
        "        # 4. final hidden states of the biLSTM is concatenated together\n",
        "        output_hidden = torch.cat((last_hidden[0,:,:], last_hidden[1,:,:]), dim=1)\n",
        "        # h_n of shape (num_layers * num_directions, batch, hidden_size)\n",
        "        \n",
        "        # 5. Dropout applied\n",
        "        output_hidden = self.dropout(output_hidden)\n",
        "        \n",
        "        # 6. Affine layer before softmax\n",
        "        output = self.fc(output_hidden)\n",
        "        \n",
        "        return output       \n",
        "    \n",
        "    \n",
        "    def load(model_path: str):\n",
        "        \"\"\" Load the model from a file.\n",
        "        @param model_path (str): path to model\n",
        "        @return model (nn.Module): model with saved parameters\n",
        "        \"\"\"\n",
        "        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "        args = params['args']\n",
        "        model = LSTM_bert(**args)\n",
        "        model.load_state_dict(params['state_dict'])\n",
        "        return model\n",
        "    \n",
        "    def save(self, path: str):\n",
        "        \"\"\" Save the model to a file.\n",
        "        @param path (str): path to the model\n",
        "        \"\"\"\n",
        "        print('save model parameters to [%s]' % path, file=sys.stderr)\n",
        "        params = {'args': dict(bert_config=self.bert_config, num_class=self.num_class, dropout_rate=self.dropout_rate),  'state_dict': self.state_dict() }\n",
        "        torch.save(params, path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFg27JlMjwIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sents(sents, pad_token):\n",
        "    \"\"\" Pad list of sentences to the longest length in the batch.\n",
        "    @param sents (list[list[str]]): list of tokenized strings\n",
        "    @param pad_token (int): pad token\n",
        "    @returns sents_padded (list[list[int]]): list of tokenized sentences with padding shape: (batch_size, max_sentence_length)\n",
        "    \"\"\"\n",
        "    sents_padded = []\n",
        "    max_len = max(len(s) for s in sents)\n",
        "    for s in sents:\n",
        "      padded = [pad_token] * max_len\n",
        "      padded[:len(s)] = s\n",
        "      sents_padded.append(padded)\n",
        "    return sents_padded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqxbxe8IkIFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sents_to_tensor(tokenizer, sents):\n",
        "    \"\"\"\n",
        "    :param tokenizer\n",
        "    :param sents: list[str], list of untokenized strings\n",
        "    \"\"\"\n",
        "    #print(\"SENTS::\", sents)\n",
        "    tokens_list = [tokenizer.tokenize(sent) for sent in sents]\n",
        "    #print(\"TOKENS_LIST\", tokens_list)\n",
        "    sents_lengths = [len(tokens) for tokens in tokens_list]\n",
        "    #print(\"SENTS_LENGTH::\", sents_lengths)\n",
        "    sents_lengths = torch.tensor(sents_lengths)\n",
        "    tokens_list_padded = pad_sents(tokens_list, '[PAD]')\n",
        "    masks = np.asarray(tokens_list_padded)!='[PAD]'\n",
        "    masks_tensor = torch.tensor(masks, dtype=torch.long)\n",
        "    tokens_id_list = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokens_list_padded]\n",
        "    sents_tensor = torch.tensor(tokens_id_list, dtype=torch.long)\n",
        "    \n",
        "    return sents_tensor, masks_tensor, sents_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm0x1ihpkKIS",
        "colab_type": "code",
        "outputId": "5f9e93d7-3808-450e-89f3-dceecd64634b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!pip install bert-tensorflow\n",
        "!pip install utils\n",
        "import bert\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "# from bert import LSTM_bert, default_bert \n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import time\n",
        "import sys\n",
        "import utils\n",
        "# from utils import batch_iter"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Requirement already satisfied: utils in /usr/local/lib/python3.6/dist-packages (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l032inz4kMV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(model, df_val, loss_func, bert_size):\n",
        "    \"\"\" validation of model during training.\n",
        "    @param model (nn.Module): the model being trained\n",
        "    @param df_val (dataframe): validation dataset, sorted in descending text length\n",
        "    @param loss_func(nn.Module): loss function\n",
        "    @return avg loss value across validation dataset\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval() #model.eval() put all layers in model in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.\n",
        "    df_val = df_val.sort_values(by='tweet_BERTbase_length', ascending=False)\n",
        "    tweet_proc_bert = list(df_val['tweet_BERT'])\n",
        "    type_label = list(df_val['type_label'])\n",
        "    \n",
        "    val_batch_size = 32\n",
        "    num_val_samples = df_val.shape[0]\n",
        "    \n",
        "    n_batch = int(np.ceil(num_val_samples/val_batch_size))\n",
        "    \n",
        "    total_loss = 0.\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in range(n_batch):\n",
        "            sents = tweet_proc_bert[i*val_batch_size: (i+1)*val_batch_size]\n",
        "            targets = torch.tensor(type_label[i*val_batch_size: (i+1)*val_batch_size],\n",
        "                                   dtype=torch.long)\n",
        "            batch_size = len(sents)\n",
        "            output = model(sents)\n",
        "            batch_loss = loss_func(output, targets)\n",
        "            total_loss += batch_loss.item()*batch_size\n",
        "    \n",
        "    if was_training:\n",
        "        model.train()\n",
        "    \n",
        "    return total_loss/num_val_samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5fQ_a6VkP9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model():\n",
        "    \n",
        "    label_name = ['neutral', 'abusive', 'hateful']\n",
        "\n",
        "    #1) Arg 1\n",
        "    bert_config_str = 'bert-base-uncased'\n",
        "    bert_size = bert_config_str.split('-')[1]\n",
        "\n",
        "    start_time = time.time()\n",
        "    print('Importing data...', file=sys.stderr)\n",
        "    \n",
        "    df_train = train_data\n",
        "    df_val = validation_data\n",
        "    train_label = dict(df_train['type_label'].value_counts())\n",
        "    label_max = float(max(train_label.values()))\n",
        "\n",
        "    print(train_label, file=sys.stderr)\n",
        "    train_label_weight = torch.tensor([label_max/train_label[i] for i in range(len(train_label))])\n",
        "    print('Done! time elapsed %.2f sec' % (time.time() - start_time), file=sys.stderr)\n",
        "    print('-' * 80, file=sys.stderr)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    print('Set up model...', file=sys.stderr)\n",
        "    lr_bert = 2e-5\n",
        "    lr = 1e-3\n",
        "    model_name = 'LSTM_bert'\n",
        "    dropout = 0.5\n",
        "    clip_grad = 1.0\n",
        "    max_epoch = 3\n",
        "\n",
        "    model = LSTM_bert(num_class=len(label_name), dropout_rate= dropout, bert_config= 'bert-base-uncased')\n",
        "    optimizer_grouped_parameters = [\n",
        "                {'params': model.bert.parameters()},\n",
        "                {'params': model.lstm.parameters(), 'lr': float(lr)},\n",
        "                {'params': model.fc.parameters(), 'lr': float(lr)}]\n",
        "\n",
        "    optimizer = BertAdam( optimizer_grouped_parameters, \n",
        "                         lr=lr_bert,\n",
        "                         max_grad_norm=clip_grad )\n",
        "            \n",
        "        \n",
        "    print('Done! time elapsed %.2f sec' % (time.time() - start_time), file=sys.stderr)\n",
        "    print('-' * 80, file=sys.stderr)\n",
        "    \n",
        "    model.train() #set model for training mode\n",
        "    criterion = torch.nn.CrossEntropyLoss(weight=train_label_weight, reduction='mean')\n",
        "    torch.save(criterion, 'loss_func')  # for later testing\n",
        "\n",
        "    train_batch_size = 64\n",
        "    valid_niter =  500\n",
        "    display_num = 10\n",
        "    \n",
        "    num_restarts = 0\n",
        "    max_num_trial = 3\n",
        "    train_iter = patience = cum_loss = report_loss = 0\n",
        "    total_samples = display_samples = epoch = 0\n",
        "    lr_decay = 0.5\n",
        "    patience_max = 3\n",
        "    valid_loss_hist = []\n",
        "    train_time = begin_time = time.time()\n",
        "    print('Begin training...')\n",
        "    \n",
        "    # while True:\n",
        "    while train_iter < 600:\n",
        "        epoch += 1\n",
        "        for sents, targets in batch_iter(df_train, batch_size=train_batch_size, shuffle=False, bert = bert_config_str):  # for each epoch\n",
        "            train_iter += 1\n",
        "            \n",
        "            batch_size = len(sents)\n",
        "            labels = torch.tensor(targets, dtype=torch.long)\n",
        "            \n",
        "            optimizer.zero_grad() #restarting the grad accumulations between mini-batches\n",
        "            output = model(sents) #pass through model\n",
        "            loss = criterion(output, labels) #calculate loss\n",
        "            loss.backward() #back prop\n",
        "            optimizer.step() #update weights\n",
        "            \n",
        "            batch_losses_val = loss.item() * batch_size\n",
        "            report_loss += batch_losses_val\n",
        "            cum_loss += batch_losses_val\n",
        "            \n",
        "            display_samples += batch_size\n",
        "            total_samples += batch_size\n",
        "            \n",
        "            if train_iter % display_num == 0:\n",
        "                print('epoch %d, iter %d, avg. loss %.2f, '\n",
        "                      'total samples %d, speed %.2f samples/sec, '\n",
        "                      'time elapsed %.2f sec' % \n",
        "                      (epoch, train_iter, report_loss / display_samples,\n",
        "                       total_samples, display_samples / (time.time() - train_time),\n",
        "                       time.time() - begin_time), file=sys.stderr)\n",
        "                train_time = time.time()\n",
        "                report_loss = display_samples = 0.\n",
        "            \n",
        "            # perform validation\n",
        "            if train_iter % valid_niter == 0:\n",
        "                print('epoch %d, iter %d, cum. loss %.2f, cum. examples %d' % \n",
        "                      (epoch, train_iter, cum_loss / total_samples, total_samples), file=sys.stderr)\n",
        "                cum_loss = total_samples = 0.\n",
        "                \n",
        "                print('begin validation ...', file=sys.stderr)\n",
        "                \n",
        "                valid_loss = validation(model, df_val, criterion, bert_size=bert_size)\n",
        "                print('validation: iter %d, loss %f' % (train_iter, valid_loss), file=sys.stderr)\n",
        "                \n",
        "#                scheduler.step(valid_loss)\n",
        "                improved_loss = len(valid_loss_hist)==0 or valid_loss < min(valid_loss_hist)\n",
        "                valid_loss_hist.append(valid_loss)\n",
        "                \n",
        "                if improved_loss:\n",
        "                    patience = 0\n",
        "                    print('save currently the best model to [%s]' % model_name +'_model.bin', file=sys.stderr)\n",
        "                    model.save(model_name+'_model.bin')\n",
        "                    \n",
        "                    # also save the optimizers' state\n",
        "                    torch.save(optimizer.state_dict(), model_name + '.optim')\n",
        "                else: #if valid loss did not improve\n",
        "                    patience += 1\n",
        "                    print('hit patience %d out of %d' % (patience, int(patience_max)), file=sys.stderr)\n",
        "\n",
        "                    if patience < int(patience_max):\n",
        "                        num_restarts += 1\n",
        "                        print('hit #%d restart out of max %d restarts' % (num_restarts, int(max_num_trial)), file=sys.stderr)\n",
        "                        if num_restarts >= int(max_num_trial):\n",
        "                            print('early termination!', file=sys.stderr)\n",
        "                            exit(0)\n",
        "                        \n",
        "                        # decay lr, and restore from previously best checkpoint\n",
        "                        lr = optimizer.param_groups[0]['lr'] * float(lr_decay)\n",
        "                        print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n",
        "                        \n",
        "                        # load model\n",
        "                        params = torch.load(model_name + '_model.bin')\n",
        "                        model.load_state_dict(params['state_dict'])\n",
        "                        \n",
        "                        print('restore parameters of the optimizers', file=sys.stderr)\n",
        "                        optimizer.load_state_dict(torch.load(model_name + '.optim'))\n",
        "                        \n",
        "                        # set new lr\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr\n",
        "                            \n",
        "                        # reset patience\n",
        "                        patience = 0\n",
        "                        \n",
        "                if epoch == max_epoch:\n",
        "                    print('reached maximum number of epochs!', file=sys.stderr)\n",
        "                    exit(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl5IoX_6oFe9",
        "colab_type": "code",
        "outputId": "ed5ebbec-d44d-4599-cf63-3a84c00d917e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importing data...\n",
            "{0: 51572, 1: 15100, 2: 3324}\n",
            "Done! time elapsed 0.00 sec\n",
            "--------------------------------------------------------------------------------\n",
            "Set up model...\n",
            "t_total value of -1 results in schedule not being applied\n",
            "Done! time elapsed 6.06 sec\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Begin training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1, iter 10, avg. loss 1.54, total samples 640, speed 19.19 samples/sec, time elapsed 33.35 sec\n",
            "epoch 1, iter 20, avg. loss 0.98, total samples 1280, speed 18.41 samples/sec, time elapsed 68.11 sec\n",
            "epoch 1, iter 30, avg. loss 0.86, total samples 1920, speed 19.77 samples/sec, time elapsed 100.48 sec\n",
            "epoch 1, iter 40, avg. loss 0.82, total samples 2560, speed 19.19 samples/sec, time elapsed 133.84 sec\n",
            "epoch 1, iter 50, avg. loss 0.82, total samples 3200, speed 19.05 samples/sec, time elapsed 167.43 sec\n",
            "epoch 1, iter 60, avg. loss 0.84, total samples 3840, speed 19.63 samples/sec, time elapsed 200.03 sec\n",
            "epoch 1, iter 70, avg. loss 0.78, total samples 4480, speed 19.39 samples/sec, time elapsed 233.04 sec\n",
            "epoch 1, iter 80, avg. loss 0.70, total samples 5120, speed 18.98 samples/sec, time elapsed 266.76 sec\n",
            "epoch 1, iter 90, avg. loss 0.63, total samples 5760, speed 18.41 samples/sec, time elapsed 301.52 sec\n",
            "epoch 1, iter 100, avg. loss 0.72, total samples 6400, speed 19.75 samples/sec, time elapsed 333.93 sec\n",
            "epoch 1, iter 110, avg. loss 0.65, total samples 7040, speed 19.71 samples/sec, time elapsed 366.39 sec\n",
            "epoch 1, iter 120, avg. loss 0.84, total samples 7680, speed 19.29 samples/sec, time elapsed 399.56 sec\n",
            "epoch 1, iter 130, avg. loss 0.74, total samples 8320, speed 19.60 samples/sec, time elapsed 432.22 sec\n",
            "epoch 1, iter 140, avg. loss 0.74, total samples 8960, speed 19.88 samples/sec, time elapsed 464.42 sec\n",
            "epoch 1, iter 150, avg. loss 0.64, total samples 9600, speed 18.90 samples/sec, time elapsed 498.28 sec\n",
            "epoch 1, iter 160, avg. loss 0.79, total samples 10240, speed 18.93 samples/sec, time elapsed 532.09 sec\n",
            "epoch 1, iter 170, avg. loss 0.73, total samples 10880, speed 19.93 samples/sec, time elapsed 564.20 sec\n",
            "epoch 1, iter 180, avg. loss 0.65, total samples 11520, speed 19.20 samples/sec, time elapsed 597.53 sec\n",
            "epoch 1, iter 190, avg. loss 0.68, total samples 12160, speed 19.50 samples/sec, time elapsed 630.34 sec\n",
            "epoch 1, iter 200, avg. loss 0.70, total samples 12800, speed 19.27 samples/sec, time elapsed 663.56 sec\n",
            "epoch 1, iter 210, avg. loss 0.75, total samples 13440, speed 19.42 samples/sec, time elapsed 696.51 sec\n",
            "epoch 1, iter 220, avg. loss 0.70, total samples 14080, speed 19.32 samples/sec, time elapsed 729.64 sec\n",
            "epoch 1, iter 230, avg. loss 0.76, total samples 14720, speed 19.20 samples/sec, time elapsed 762.98 sec\n",
            "epoch 1, iter 240, avg. loss 0.59, total samples 15360, speed 18.47 samples/sec, time elapsed 797.63 sec\n",
            "epoch 1, iter 250, avg. loss 0.75, total samples 16000, speed 19.22 samples/sec, time elapsed 830.93 sec\n",
            "epoch 1, iter 260, avg. loss 0.83, total samples 16640, speed 19.00 samples/sec, time elapsed 864.62 sec\n",
            "epoch 1, iter 270, avg. loss 0.68, total samples 17280, speed 18.91 samples/sec, time elapsed 898.47 sec\n",
            "epoch 1, iter 280, avg. loss 0.72, total samples 17920, speed 19.61 samples/sec, time elapsed 931.11 sec\n",
            "epoch 1, iter 290, avg. loss 0.59, total samples 18560, speed 20.35 samples/sec, time elapsed 962.57 sec\n",
            "epoch 1, iter 300, avg. loss 0.77, total samples 19200, speed 19.33 samples/sec, time elapsed 995.68 sec\n",
            "epoch 1, iter 310, avg. loss 0.61, total samples 19840, speed 19.37 samples/sec, time elapsed 1028.72 sec\n",
            "epoch 1, iter 320, avg. loss 0.61, total samples 20480, speed 20.13 samples/sec, time elapsed 1060.51 sec\n",
            "epoch 1, iter 330, avg. loss 0.67, total samples 21120, speed 19.94 samples/sec, time elapsed 1092.60 sec\n",
            "epoch 1, iter 340, avg. loss 0.62, total samples 21760, speed 19.54 samples/sec, time elapsed 1125.37 sec\n",
            "epoch 1, iter 350, avg. loss 0.79, total samples 22400, speed 20.11 samples/sec, time elapsed 1157.19 sec\n",
            "epoch 1, iter 360, avg. loss 0.67, total samples 23040, speed 18.90 samples/sec, time elapsed 1191.04 sec\n",
            "epoch 1, iter 370, avg. loss 0.72, total samples 23680, speed 19.74 samples/sec, time elapsed 1223.46 sec\n",
            "epoch 1, iter 380, avg. loss 0.72, total samples 24320, speed 19.22 samples/sec, time elapsed 1256.77 sec\n",
            "epoch 1, iter 390, avg. loss 0.68, total samples 24960, speed 19.40 samples/sec, time elapsed 1289.76 sec\n",
            "epoch 1, iter 400, avg. loss 0.62, total samples 25600, speed 18.90 samples/sec, time elapsed 1323.63 sec\n",
            "epoch 1, iter 410, avg. loss 0.73, total samples 26240, speed 18.95 samples/sec, time elapsed 1357.41 sec\n",
            "epoch 1, iter 420, avg. loss 0.86, total samples 26880, speed 18.69 samples/sec, time elapsed 1391.65 sec\n",
            "epoch 1, iter 430, avg. loss 0.92, total samples 27520, speed 18.84 samples/sec, time elapsed 1425.63 sec\n",
            "epoch 1, iter 440, avg. loss 0.62, total samples 28160, speed 17.61 samples/sec, time elapsed 1461.96 sec\n",
            "epoch 1, iter 450, avg. loss 0.64, total samples 28800, speed 18.33 samples/sec, time elapsed 1496.89 sec\n",
            "epoch 1, iter 460, avg. loss 0.61, total samples 29440, speed 18.51 samples/sec, time elapsed 1531.46 sec\n",
            "epoch 1, iter 470, avg. loss 0.64, total samples 30080, speed 17.83 samples/sec, time elapsed 1567.35 sec\n",
            "epoch 1, iter 480, avg. loss 0.67, total samples 30720, speed 18.74 samples/sec, time elapsed 1601.51 sec\n",
            "epoch 1, iter 490, avg. loss 0.68, total samples 31360, speed 18.27 samples/sec, time elapsed 1636.54 sec\n",
            "epoch 1, iter 500, avg. loss 0.61, total samples 32000, speed 18.85 samples/sec, time elapsed 1670.50 sec\n",
            "epoch 1, iter 500, cum. loss 0.73, cum. examples 32000\n",
            "begin validation ...\n",
            "validation: iter 500, loss 0.622932\n",
            "save currently the best model to [LSTM_bert]_model.bin\n",
            "save model parameters to [LSTM_bert_model.bin]\n",
            "epoch 1, iter 510, avg. loss 0.80, total samples 640, speed 3.49 samples/sec, time elapsed 1853.93 sec\n",
            "epoch 1, iter 520, avg. loss 0.66, total samples 1280, speed 18.11 samples/sec, time elapsed 1889.27 sec\n",
            "epoch 1, iter 530, avg. loss 0.59, total samples 1920, speed 18.87 samples/sec, time elapsed 1923.19 sec\n",
            "epoch 1, iter 540, avg. loss 0.58, total samples 2560, speed 19.37 samples/sec, time elapsed 1956.24 sec\n",
            "epoch 1, iter 550, avg. loss 0.75, total samples 3200, speed 18.93 samples/sec, time elapsed 1990.06 sec\n",
            "epoch 1, iter 560, avg. loss 0.58, total samples 3840, speed 19.20 samples/sec, time elapsed 2023.39 sec\n",
            "epoch 1, iter 570, avg. loss 0.73, total samples 4480, speed 19.25 samples/sec, time elapsed 2056.64 sec\n",
            "epoch 1, iter 580, avg. loss 0.59, total samples 5120, speed 19.08 samples/sec, time elapsed 2090.18 sec\n",
            "epoch 1, iter 590, avg. loss 0.62, total samples 5760, speed 18.55 samples/sec, time elapsed 2124.68 sec\n",
            "epoch 1, iter 600, avg. loss 0.57, total samples 6400, speed 19.62 samples/sec, time elapsed 2157.31 sec\n",
            "epoch 1, iter 610, avg. loss 0.65, total samples 7040, speed 19.78 samples/sec, time elapsed 2189.67 sec\n",
            "epoch 1, iter 620, avg. loss 0.50, total samples 7680, speed 18.89 samples/sec, time elapsed 2223.55 sec\n",
            "epoch 1, iter 630, avg. loss 0.65, total samples 8320, speed 19.08 samples/sec, time elapsed 2257.09 sec\n",
            "epoch 1, iter 640, avg. loss 0.63, total samples 8960, speed 19.61 samples/sec, time elapsed 2289.72 sec\n",
            "epoch 1, iter 650, avg. loss 0.64, total samples 9600, speed 19.42 samples/sec, time elapsed 2322.68 sec\n",
            "epoch 1, iter 660, avg. loss 0.60, total samples 10240, speed 18.86 samples/sec, time elapsed 2356.61 sec\n",
            "epoch 1, iter 670, avg. loss 0.65, total samples 10880, speed 19.16 samples/sec, time elapsed 2390.02 sec\n",
            "epoch 1, iter 680, avg. loss 0.49, total samples 11520, speed 19.48 samples/sec, time elapsed 2422.87 sec\n",
            "epoch 1, iter 690, avg. loss 0.49, total samples 12160, speed 19.37 samples/sec, time elapsed 2455.92 sec\n",
            "epoch 1, iter 700, avg. loss 0.71, total samples 12800, speed 19.77 samples/sec, time elapsed 2488.29 sec\n",
            "epoch 1, iter 710, avg. loss 0.64, total samples 13440, speed 19.70 samples/sec, time elapsed 2520.77 sec\n",
            "epoch 1, iter 720, avg. loss 0.52, total samples 14080, speed 18.96 samples/sec, time elapsed 2554.54 sec\n",
            "epoch 1, iter 730, avg. loss 0.59, total samples 14720, speed 19.53 samples/sec, time elapsed 2587.31 sec\n",
            "epoch 1, iter 740, avg. loss 0.57, total samples 15360, speed 19.36 samples/sec, time elapsed 2620.37 sec\n",
            "epoch 1, iter 750, avg. loss 0.61, total samples 16000, speed 18.75 samples/sec, time elapsed 2654.50 sec\n",
            "epoch 1, iter 760, avg. loss 0.63, total samples 16640, speed 19.53 samples/sec, time elapsed 2687.27 sec\n",
            "epoch 1, iter 770, avg. loss 0.50, total samples 17280, speed 20.04 samples/sec, time elapsed 2719.21 sec\n",
            "epoch 1, iter 780, avg. loss 0.72, total samples 17920, speed 19.50 samples/sec, time elapsed 2752.02 sec\n",
            "epoch 1, iter 790, avg. loss 0.65, total samples 18560, speed 19.27 samples/sec, time elapsed 2785.23 sec\n",
            "epoch 1, iter 800, avg. loss 0.65, total samples 19200, speed 19.07 samples/sec, time elapsed 2818.79 sec\n",
            "epoch 1, iter 810, avg. loss 0.68, total samples 19840, speed 19.30 samples/sec, time elapsed 2851.96 sec\n",
            "epoch 1, iter 820, avg. loss 0.54, total samples 20480, speed 19.09 samples/sec, time elapsed 2885.48 sec\n",
            "epoch 1, iter 830, avg. loss 0.61, total samples 21120, speed 19.44 samples/sec, time elapsed 2918.40 sec\n",
            "epoch 1, iter 840, avg. loss 0.70, total samples 21760, speed 19.97 samples/sec, time elapsed 2950.45 sec\n",
            "epoch 1, iter 850, avg. loss 0.60, total samples 22400, speed 19.12 samples/sec, time elapsed 2983.91 sec\n",
            "epoch 1, iter 860, avg. loss 0.60, total samples 23040, speed 19.59 samples/sec, time elapsed 3016.58 sec\n",
            "epoch 1, iter 870, avg. loss 0.68, total samples 23680, speed 19.34 samples/sec, time elapsed 3049.68 sec\n",
            "epoch 1, iter 880, avg. loss 0.61, total samples 24320, speed 19.89 samples/sec, time elapsed 3081.85 sec\n",
            "epoch 1, iter 890, avg. loss 0.65, total samples 24960, speed 19.49 samples/sec, time elapsed 3114.69 sec\n",
            "epoch 1, iter 900, avg. loss 0.62, total samples 25600, speed 19.61 samples/sec, time elapsed 3147.33 sec\n",
            "epoch 1, iter 910, avg. loss 0.57, total samples 26240, speed 18.78 samples/sec, time elapsed 3181.42 sec\n",
            "epoch 1, iter 920, avg. loss 0.64, total samples 26880, speed 19.47 samples/sec, time elapsed 3214.29 sec\n",
            "epoch 1, iter 930, avg. loss 0.56, total samples 27520, speed 19.09 samples/sec, time elapsed 3247.81 sec\n",
            "epoch 1, iter 940, avg. loss 0.52, total samples 28160, speed 18.76 samples/sec, time elapsed 3281.94 sec\n",
            "epoch 1, iter 950, avg. loss 0.51, total samples 28800, speed 19.48 samples/sec, time elapsed 3314.79 sec\n",
            "epoch 1, iter 960, avg. loss 0.72, total samples 29440, speed 19.36 samples/sec, time elapsed 3347.85 sec\n",
            "epoch 1, iter 970, avg. loss 0.60, total samples 30080, speed 19.67 samples/sec, time elapsed 3380.39 sec\n",
            "epoch 1, iter 980, avg. loss 0.60, total samples 30720, speed 19.57 samples/sec, time elapsed 3413.10 sec\n",
            "epoch 1, iter 990, avg. loss 0.58, total samples 31360, speed 18.86 samples/sec, time elapsed 3447.03 sec\n",
            "epoch 1, iter 1000, avg. loss 0.53, total samples 32000, speed 19.44 samples/sec, time elapsed 3479.95 sec\n",
            "epoch 1, iter 1000, cum. loss 0.61, cum. examples 32000\n",
            "begin validation ...\n",
            "validation: iter 1000, loss 0.559336\n",
            "save currently the best model to [LSTM_bert]_model.bin\n",
            "save model parameters to [LSTM_bert_model.bin]\n",
            "epoch 1, iter 1010, avg. loss 0.66, total samples 640, speed 3.51 samples/sec, time elapsed 3662.41 sec\n",
            "epoch 1, iter 1020, avg. loss 0.59, total samples 1280, speed 19.87 samples/sec, time elapsed 3694.61 sec\n",
            "epoch 1, iter 1030, avg. loss 0.66, total samples 1920, speed 19.41 samples/sec, time elapsed 3727.59 sec\n",
            "epoch 1, iter 1040, avg. loss 0.51, total samples 2560, speed 19.60 samples/sec, time elapsed 3760.25 sec\n",
            "epoch 1, iter 1050, avg. loss 0.77, total samples 3200, speed 18.89 samples/sec, time elapsed 3794.12 sec\n",
            "epoch 1, iter 1060, avg. loss 0.58, total samples 3840, speed 19.46 samples/sec, time elapsed 3827.00 sec\n",
            "epoch 1, iter 1070, avg. loss 0.51, total samples 4480, speed 19.73 samples/sec, time elapsed 3859.43 sec\n",
            "epoch 1, iter 1080, avg. loss 0.56, total samples 5120, speed 19.53 samples/sec, time elapsed 3892.21 sec\n",
            "epoch 1, iter 1090, avg. loss 0.59, total samples 5760, speed 18.57 samples/sec, time elapsed 3926.67 sec\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJoD64RlxOH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function for plotting confusion matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, path='cm', cmap=plt.cm.Reds):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    pickle.dump(cm, open(path, 'wb'))\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHOwK1jWBuS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model():\n",
        "\n",
        "    label_name = ['neutral', 'abusive', 'hateful']\n",
        "    model_name = 'LSTM_bert'\n",
        "    \n",
        "\n",
        "    prefix = 'LSTM_bert'\n",
        "    bert_config_str = 'bert-base-uncased'\n",
        "    bert_size = bert_config_str.split('-')[1]\n",
        "\n",
        "    print('load best model...')\n",
        "    \n",
        "    model = LSTM_bert.load(model_name+'_model.bin')\n",
        "    print(\"Model loaded\")\n",
        "    model.eval()\n",
        "\n",
        "    df_test = test_data\n",
        "    df_test = df_test.sort_values(by='tweet_BERTbase_length', ascending=False)\n",
        "\n",
        "    test_batch_size = 32\n",
        "    n_batch = int(np.ceil(df_test.shape[0]/test_batch_size))\n",
        "\n",
        "    cn_loss = torch.load('loss_func')\n",
        "    tweet_proc_bert = list(df_test['tweet_BERT'])\n",
        "    type_label = list(df_test['type_label'])\n",
        "\n",
        "    test_loss = 0.\n",
        "    prediction = []\n",
        "    prob = []\n",
        "\n",
        "    softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_batch):\n",
        "            sents = tweet_proc_bert[i*test_batch_size: (i+1)*test_batch_size]\n",
        "            targets = torch.tensor(type_label[i * test_batch_size: (i + 1) * test_batch_size],\n",
        "                                   dtype=torch.long)\n",
        "            batch_size = len(sents)\n",
        "\n",
        "            pre_softmax = model(sents)\n",
        "            batch_loss = cn_loss(pre_softmax, targets)\n",
        "            test_loss += batch_loss.item()*batch_size\n",
        "            prob_batch = softmax(pre_softmax)\n",
        "            prob.append(prob_batch)\n",
        "\n",
        "            prediction.extend([t.item() for t in list(torch.argmax(prob_batch, dim=1))])\n",
        "\n",
        "    prob = torch.cat(tuple(prob), dim=0)\n",
        "    loss = test_loss/df_test.shape[0]\n",
        "\n",
        "    pickle.dump([label_name[i] for i in prediction], open(prefix+'_test_prediction', 'wb'))\n",
        "    pickle.dump(prob.data.cpu().numpy(), open(prefix + '_test_prediction_prob', 'wb'))\n",
        "    \n",
        "    accuracy = accuracy_score(df_test['type_label'].values, prediction)\n",
        "    f1 = f1_score(df_test['type_label'].values, prediction, average='weighted')\n",
        "    f1s = {}\n",
        "    for i in range(len(label_name)):\n",
        "        prediction_ = [1 if pred == i else 0 for pred in prediction]\n",
        "        true_ = [1 if label == i else 0 for label in df_test.type_label.values]\n",
        "        f1s.update({label_name[i]: f1_score(true_, prediction_)})\n",
        "\n",
        "    cm = plot_confusion_matrix(list(df_test['type_label'].values), prediction, label_name, normalize=False, path=prefix+'_confusion_matrix', title='Confusion matrix for BERT+LSTM')\n",
        "    plt.savefig(prefix+'_confusion_matrix', format='png')\n",
        "\n",
        "    verbose = True\n",
        "    if verbose:\n",
        "        print('loss: %.2f' % loss)\n",
        "        print('accuracy: %.2f' % accuracy)\n",
        "        print('f1 score: %.2f' % f1)\n",
        "        print('-' * 80)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRf7bN-HF3iD",
        "colab_type": "code",
        "outputId": "e505af2a-d3ea-4330-9fd4-ac51600aced8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "test_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load best model...\n",
            "Model loaded\n",
            "loss: 0.59\n",
            "accuracy: 0.88\n",
            "f1 score: 0.89\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSCuoqBdKyys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}