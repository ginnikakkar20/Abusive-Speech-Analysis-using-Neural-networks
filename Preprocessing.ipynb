{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing & ngram generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIJ2ba-twZnb",
        "colab_type": "text"
      },
      "source": [
        "# PreProcess Twitter Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDpZBQvBvxLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import re\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from copy import deepcopy\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer \n",
        "\n",
        "import preprocessor as p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dSfZ9s7v4bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayAHmG_2v5Gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(\"/content/drive/My Drive/WebProject/labeledTrainData.csv\", delimiter=\"\\t\",names=[\"tweet\", \"label\", \"labelValue\"])\n",
        "train[\"label\"].replace({\"spam\": \"neutral\", \"normal\": \"neutral\"}, inplace=True)\n",
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93XM-c3HwBVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def expand_hashtags(tweet):\n",
        "    ws.load()\n",
        "    new_tweet = \"\" # output    \n",
        "    tokens = tweet.split(' ')    \n",
        "    for t in tokens:\n",
        "        if t[0] == '#': # this is a hashtag, parse it\n",
        "            hashtags_tokens = ws.segment(t)\n",
        "            exp = \" \".join(hashtags_tokens)\n",
        "            new_tweet += exp\n",
        "        else: # Just append the word\n",
        "            new_tweet += t\n",
        "        new_tweet += \" \"\n",
        "\n",
        "    return new_tweet \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRy4az-gwCSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_tweet(tweet):\n",
        "\n",
        "    # 1. Remove URLs, emojis, mentions, smileys using tweet-preprocessor library\n",
        "    # should not remove hashtags as hashtags contains important tweet content.\n",
        "    \n",
        "    #p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION,p.OPT.RESERVED,\tp.OPT.SMILEY,p.OPT.NUMBER)\n",
        "    clean = p.clean(tweet)\n",
        "    # 2. Remove HTML tags using Beautiful soup library\n",
        "    no_tag = BeautifulSoup(clean).get_text()\n",
        "\n",
        "    # 3. Expanding hashtags.\n",
        "    \n",
        "    #tweets_expanded = expand_hashtags(no_tag)\n",
        "\n",
        "    # 4. Remove non letter char using re\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\",\" \", no_tag)\n",
        "    #letters_only = re.sub(\"[^a-zA-Z]\",\" \", tweets_expanded)\n",
        "    \n",
        "    # 5. Convert to lower case\n",
        "    lower_case = letters_only.lower()\n",
        "    words = lower_case.split()\n",
        "\n",
        "    new_words = []\n",
        "    ps = PorterStemmer() \n",
        "    for w in words:\n",
        "        new_words.append(ps.stem(w))\n",
        "\n",
        "    # 5. Remove stop words\n",
        "    stops = set(stopwords.words(\"english\")) \n",
        "    final_words = [w for w in new_words if not w in stops]\n",
        "\n",
        "    return(\" \".join( final_words ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTzMPkBAwL67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_tweet = []\n",
        "\n",
        "num_tweets = train[\"tweet\"].size\n",
        "# print(num_tweets)\n",
        "for i in range(num_tweets):\n",
        "    cleaned_tweet.append(preprocess_tweet(train[\"tweet\"][i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91X83VWIwVC2",
        "colab_type": "text"
      },
      "source": [
        "### **Downloading preprocessed data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGkqggb1wUNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " df_data = pd.DataFrame(cleaned_tweet, columns=['tweet'] )\n",
        " df_data['label'] = train['label']\n",
        " df_data = df_data.reindex(columns=['tweet','label'])\n",
        " df_data.to_csv('preprocessed_tweets.csv', index = False)\n",
        "\n",
        "# print(df_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjbAkuj9xd_I",
        "colab_type": "text"
      },
      "source": [
        "## Ngram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDZYo3_2xhef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import nltk, re, string, collections\n",
        "import operator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU5u-_JTxlE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sentiments\n",
        "train_sentiment = train[\"label\"].values\n",
        "# print(cleaned_tweet[0])\n",
        "# print(train_sentiment)\n",
        "abusive = []\n",
        "hateful = []\n",
        "for x in range (train_sentiment.size):\n",
        "  if train_sentiment[x] == 'abusive':\n",
        "    abusive.append(cleaned_tweet[x])\n",
        "  else: \n",
        "    if train_sentiment[x] == 'hateful':\n",
        "      hateful.append(cleaned_tweet[x])\n",
        "\n",
        "len(abusive)\n",
        "len(hateful)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMPck5wqxlzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_ngrams(tweets, n_size):\n",
        "  x = []\n",
        "  c = Counter()\n",
        "  for t in tweets:\n",
        "    tokenized = t.split()\n",
        "    esBigrams = ngrams(tokenized, n_size)\n",
        "    esBigramFreq = collections.Counter(esBigrams)\n",
        "    c = c + esBigramFreq\n",
        "\n",
        "  d = dict(c)\n",
        "  sorted_d = sorted(d.items(), key=operator.itemgetter(1), reverse=True)\n",
        "  print(type(sorted_d))\n",
        "  return sorted_d  \n",
        "  \n",
        "\n",
        "uni_abusive = create_ngrams(abusive,1)\n",
        "bi_abusive =  create_ngrams(abusive,2)\n",
        "\n",
        "uni_hateful = create_ngrams(hateful,1)\n",
        "bi_hateful =  create_ngrams(hateful,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9Qfk2eBxq5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(uni_abusive))\n",
        "print(uni_abusive[:10])\n",
        "\n",
        "print(len(bi_abusive))\n",
        "print(bi_abusive[:10])\n",
        "\n",
        "print(len(uni_hateful))\n",
        "print(uni_hateful[:10])\n",
        "\n",
        "print(len(bi_hateful))\n",
        "print(bi_hateful[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}